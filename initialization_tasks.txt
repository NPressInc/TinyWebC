# Docker-based Initialization and Integration Testing Plan

## Overview
Create a Docker Compose system for easy initialization and testing:
- **Pluggable discovery mechanism**: Support multiple discovery backends (Tailscale, DNS pattern, static config)
- **Tailscale mode (initial)**: Use Tailscale sidecars for peer discovery via API
- **DNS pattern mode**: Support DNS-based discovery for any DNS service (Cloudflare, DuckDNS, etc.)
- Dynamic peer addition: When receiving messages from unknown peers
- Fixed ports: All nodes use `api_port=8000`, `gossip_port=9000` (isolated network namespaces)
- All nodes equal (no primary/bootstrap)
- Discovery flow: Node2 queries discovery backend → finds node1 → sends `CONTENT_NODE_REGISTRATION` → node1 discovers node2
- Fully containerized: No host setup required (for Tailscale mode, just provide `TS_AUTHKEY`)

## Completed Tasks
- [x] Move init.c, init.h, init_tool.c to scripts/ directory
- [x] Update CMakeLists.txt with new paths
- [x] Update test file includes
- [x] Update config paths in test files
- [x] Standardize database name to `tinyweb.db`
- [x] Standardize node ID format to 2 digits (`node_01`)
- [x] Standardize state directory to `state/` (no node subdirectories)
- [x] Update directory structure to use `storage/` instead of `blockchain/`
- [x] **Task 1.5**: Update Configuration System - Added discovery_mode, hostname_prefix, dns_domain fields to NodeConfig
- [x] **Task 1.8**: Update Database Schema - Added nodes table with discovery parameters, schema migration from v1 to v2
- [x] **Task 1.7**: Integrate Discovery into Main Application - Added discover_peers() call before bootstrap_known_peers()
- [x] **Store Node Configuration in Database** - Added nodes_insert_or_update() calls in init.c and main.c to persist node configuration
- [x] **Task 2.1**: Dynamic Peer Addition on Message Receive - Added discover_peer_from_source() to handle unknown peers dynamically
- [x] **Task 2.2**: Node Announcement Helper Function - Implemented send_node_announcement() to broadcast node info to peers
- [x] **Task 2.3**: Handle Node Announcement Messages - Implemented handle_node_announcement() to process incoming announcements

---

## Phase 1: Discovery Infrastructure (CORE - Must implement first)

**Goal**: Implement pluggable peer discovery system that works with Tailscale, DNS patterns, and static configs.

**Files to create/modify**:
- `src/packages/discovery/discovery.h` (NEW)
- `src/packages/discovery/discovery.c` (NEW)
- `src/packages/discovery/tailscale_discovery.c` (NEW)
- `src/packages/discovery/dns_pattern_discovery.c` (NEW)
- `src/packages/discovery/static_discovery.c` (NEW)
- `src/packages/utils/config.h` (MODIFY)
- `src/packages/utils/config.c` (MODIFY)
- `src/main.c` (MODIFY)

**Testing checkpoint**: After Phase 1, you should be able to:
- Compile with discovery module
- Load discovery_mode from config
- Call discovery functions (even if they don't find peers yet)
- Verify nodes table exists in database after initialization
- Verify node configuration is stored in database with discovery_mode, hostname_prefix, dns_domain
- Verify schema migration from version 1 to 2 works correctly

### 1.1 Create Discovery Module Structure

**File**: `src/packages/discovery/discovery.h` (CREATE)
- [ ] Create header file with:
  ```c
  #ifndef TW_DISCOVERY_H
  #define TW_DISCOVERY_H
  
  #include "packages/comm/gossip/gossip.h"
  #include "packages/utils/config.h"
  
  typedef enum {
      DISCOVERY_TAILSCALE,
      DISCOVERY_DNS_PATTERN,
      DISCOVERY_STATIC,
      DISCOVERY_NONE
  } DiscoveryMode;
  
  typedef int (*PeerDiscoveryFunc)(GossipService* service, const NodeConfig* config);
  
  int discover_peers(GossipService* service, const NodeConfig* config);
  #endif
  ```

**File**: `src/packages/discovery/discovery.c` (CREATE)
- [ ] Create router function `discover_peers()`:
  - Read `config->discovery_mode`
  - Switch on mode and call appropriate discovery function
  - Return 0 on success, -1 on failure (graceful fallback)

### 1.2 Implement Tailscale Discovery

**File**: `src/packages/discovery/tailscale_discovery.c` (CREATE)
- [ ] Function: `discover_tailscale_peers(GossipService* service, const NodeConfig* config)`
- [ ] Query Tailscale API: `tailscale status --json` (runs in container, shares network with sidecar)
- [ ] Parse JSON to find devices matching `{hostname_prefix}*` pattern (from config)
  - Pattern examples: `tw_node*` matches `tw_node01`, `tw_node22`, `tw_node03`, etc.
  - Pattern examples: `smithfam_tw_node*` matches `smithfam_tw_node01`, `smithfam_tw_node22`, etc.
- [ ] For each discovered device:
  - Extract hostname (e.g., `tw_node01`, `tw_node22`, `tw_node03` - 2 digits, zero-padded)
  - Use fixed gossip port: `9000` (hardcoded, all nodes use same port)
  - Skip self (compare discovered hostname to own hostname from config)
- [ ] Add discovered peers via `gossip_service_add_peer(service, hostname, 9000)`
- [ ] Store in database via `gossip_peers_add_or_update(hostname, 9000, 0, NULL)`
- [ ] Add retry logic: Wait for Tailscale to be connected (check `tailscale status` returns valid JSON)
  - Retry up to 30 times with 2-second delays (60 seconds total)
  - Log warnings if Tailscale not ready
- [ ] Return 0 on success, -1 on failure (graceful fallback)

**Dependencies**: 
- `gossip_service_add_peer()` from `src/packages/comm/gossip/gossip.c`
- `gossip_peers_add_or_update()` from `src/packages/sql/gossip_peers.c`
- `tailscale` CLI must be available in container (installed in Dockerfile)

### 1.3 Implement DNS Pattern Discovery

**File**: `src/packages/discovery/dns_pattern_discovery.c` (CREATE)
- [ ] Function: `discover_dns_pattern_peers(GossipService* service, const NodeConfig* config)`
- [ ] Read DNS pattern from config:
  - If `dns_pattern.pattern` is provided, use it directly (e.g., `"tw_node*.tinyweb.win"`)
  - Otherwise, build pattern from `hostname_prefix` + `domain`: `"{hostname_prefix}*.{domain}"` (e.g., `"tw_node*.duckdns.org"` or `"smithfam_tw_node*.duckdns.org"`)
- [ ] Extract `hostname_prefix` and `domain` from config for pattern building
- [ ] Query DNS for all A/AAAA records matching pattern:
  - Pattern examples: `tw_node*.duckdns.org` matches `tw_node01.duckdns.org`, `tw_node22.duckdns.org`, `tw_node03.duckdns.org`, etc.
  - Since DNS wildcard enumeration may not be supported, iterate through possible hostnames:
    - Try `{hostname_prefix}01.{domain}`, `{hostname_prefix}02.{domain}`, ... up to `{hostname_prefix}99.{domain}`
    - For each that resolves (e.g., `tw_node01.duckdns.org`, `tw_node22.duckdns.org`):
      - Use fixed gossip port: `9000`
      - Skip self (compare hostname to own hostname from config)
      - Add peer via `gossip_service_add_peer(service, hostname, 9000)`
      - Store in database via `gossip_peers_add_or_update(hostname, 9000, 0, NULL)`
- [ ] Works with any DNS service: Cloudflare, DuckDNS, custom domains, etc.
- [ ] Note: Since DNS wildcard enumeration is rarely supported, iteration through 01-99 is the practical approach

**Dependencies**:
- Standard DNS resolution via `getaddrinfo()` (no external dependencies)

### 1.4 Implement Static Discovery

**File**: `src/packages/discovery/static_discovery.c` (CREATE)
- [ ] Function: `discover_static_peers(GossipService* service, const NodeConfig* config)`
- [ ] Read peers from `config->peers` array (from node-specific config)
- [ ] For each peer in config:
  - Parse hostname:port format (default port 9000 if not specified)
  - Skip self (compare hostname to own hostname from config)
  - Add peer via `gossip_service_add_peer(service, hostname, port)`
  - Store in database via `gossip_peers_add_or_update(hostname, port, 0, NULL)`
- [ ] Works with any DNS-resolvable hostname: DuckDNS, direct IPs, custom domains, etc.
- [ ] No retry logic needed (static config is always available)

**Dependencies**:
- Standard DNS resolution via `getaddrinfo()`

### 1.5 Update Configuration System (COMPLETED)

**File**: `src/packages/utils/config.h` (MODIFY)
- [x] Add field to `NodeConfig` struct:
  ```c
  char discovery_mode[32];        // 'tailscale', 'dns_pattern', 'static', or 'none'
  char hostname_prefix[64];       // For discovery pattern matching
  char dns_domain[256];           // For DNS pattern mode
  ```

**File**: `src/packages/utils/config.c` (MODIFY)
- [x] Update `config_load_node_from_network_config()` to read:
  - `discovery_mode` from `docker.discovery.mode` field
  - `hostname_prefix` from `docker.discovery.hostname_prefix` field
  - `dns_domain` from `docker.discovery.dns_pattern.domain` field (if DNS pattern mode)
- [x] Update `config_set_defaults()` to set default discovery_mode to "static"
- [x] Update `config_merge()` to merge discovery fields
- [x] Update `config_load_from_env()` to support `TINYWEB_DISCOVERY_MODE`, `TINYWEB_HOSTNAME_PREFIX`, `TINYWEB_DNS_DOMAIN` environment variables

### 1.7 Resolve Schema Duplication (COMPLETED)

**Status**: ✅ **COMPLETED** - `schema.c` is now the canonical schema file (renamed from `gossip_store.c`).

**Completed refactoring**:
- [x] Deleted old `src/packages/sql/schema.c` (outdated)
- [x] Renamed `src/packages/sql/gossip_store.c` → `src/packages/sql/schema.c`
- [x] Renamed `src/packages/sql/gossip_store.h` → `src/packages/sql/schema.h` (replaced old schema.h)
- [x] Updated all includes: `gossip_store.h` → `schema.h`
- [x] Updated CMakeLists.txt: Changed `gossip_store.c` to `schema.c` in init_tool sources
- [x] Updated `scripts/init.c`: Removed calls to old `schema_create_all_tables()` (now uses `gossip_store_init()`)
- [x] Updated test files: Replaced `schema_create_all_tables()` calls with `gossip_store_init()`
- [x] Added schema versioning functions to `schema.c`: `schema_check_version()`, `schema_set_version()`, `schema_migrate()`

**Current state**: 
- `schema.c` is the canonical schema file with up-to-date table definitions (INTEGER timestamps, transaction_id columns)
- `gossip_store_init()` creates all tables and indexes
- Schema versioning functions are available for future migrations
- All code now uses unified schema initialization via `gossip_store_init()`

### 1.8 Update Database Schema for Discovery Parameters (COMPLETED)

**File**: `src/packages/sql/schema.c` (MODIFY)
- [x] Add `nodes` table to store current node's configuration:
  ```sql
  CREATE TABLE IF NOT EXISTS nodes (
      node_id TEXT PRIMARY KEY,
      node_name TEXT NOT NULL,
      hostname TEXT NOT NULL,
      gossip_port INTEGER NOT NULL,
      api_port INTEGER NOT NULL,
      discovery_mode TEXT NOT NULL,  -- 'tailscale', 'dns_pattern', 'static'
      hostname_prefix TEXT,           -- For discovery pattern matching
      dns_domain TEXT,                -- For DNS pattern mode
      created_at INTEGER NOT NULL DEFAULT (strftime('%s','now')),
      updated_at INTEGER NOT NULL DEFAULT (strftime('%s','now'))
  );
  ```
- [x] Add index: `CREATE INDEX IF NOT EXISTS idx_nodes_node_id ON nodes(node_id);`
- [x] Add SQL statements for node CRUD operations:
  - `SQL_INSERT_OR_UPDATE_NODE` - Insert or update node config (with ON CONFLICT)
  - `SQL_SELECT_NODE_BY_ID` - Get node config by node_id
- [x] Add nodes table creation to `gossip_store_init()` function (in schema.c)
  - Added `GOSSIP_CREATE_NODES` SQL definition
  - Added `GOSSIP_CREATE_INDEX_NODES_NODE_ID` SQL definition
  - Added both to the `schema_statements[]` array in `gossip_store_init()`
- [x] Update `schema_migrate()` to handle schema version 1 → 2 (add nodes table)
  - Added migration path: `if (from_version == 1 && to_version >= 2) { ... }`
  - Creates nodes table and index, sets version to 2
- [x] Update `CURRENT_SCHEMA_VERSION` from 1 to 2 in `schema.h` and `schema.c`

**File**: `src/packages/sql/schema.h` (MODIFY)
- [x] Add extern declarations for SQL constants:
  - `extern const char* SQL_INSERT_OR_UPDATE_NODE;`
  - `extern const char* SQL_SELECT_NODE_BY_ID;`
- [x] Add function declarations:
  - `int nodes_insert_or_update(const char* node_id, const char* node_name, const char* hostname, uint16_t gossip_port, uint16_t api_port, const char* discovery_mode, const char* hostname_prefix, const char* dns_domain);`
  - `int nodes_get_by_id(const char* node_id, char* node_name, size_t name_len, char* hostname, size_t hostname_len, uint16_t* gossip_port, uint16_t* api_port, char* discovery_mode, size_t mode_len, char* hostname_prefix, size_t prefix_len, char* dns_domain, size_t domain_len);`
- [x] Implement `nodes_insert_or_update()` function in schema.c
- [x] Implement `nodes_get_by_id()` function in schema.c

**File**: `src/packages/sql/gossip_peers.c` (MODIFY - optional enhancement)
- [ ] Consider adding `discovery_mode` column to `gossip_peers` table to track how each peer was discovered:
  ```sql
  ALTER TABLE gossip_peers ADD COLUMN discovery_mode TEXT;
  ```
  - This allows tracking whether peer was discovered via Tailscale, DNS pattern, static config, or dynamic addition
  - Update `gossip_peers_add_or_update()` to accept and store `discovery_mode` parameter

**File**: `scripts/init.c` (MODIFY)
- [x] After initializing node, store node configuration in database:
  - Call `nodes_insert_or_update()` with node_id, node_name, hostname, ports, discovery_mode, hostname_prefix, dns_domain
  - This persists node's own configuration in database for future reference
  - Uses defaults for discovery parameters (discovery_mode="static") since they're network-level and will be updated when main.c loads the config

**File**: `src/main.c` (MODIFY)
- [x] After loading config, store node configuration in database:
  - Call `nodes_insert_or_update()` with loaded config values (node_id, node_name, hostname, gossip_port, api_port, discovery_mode, hostname_prefix, dns_domain)
  - This ensures database has current node configuration even if config file is missing later
  - Added call after `initialize_storage()` succeeds, before `start_gossip_service()`

### 1.6 Update Database Schema for Discovery Parameters (COMPLETED - See 1.8)

**Note**: This section is a duplicate of section 1.8. All tasks have been completed in section 1.8.

**File**: `src/packages/sql/gossip_peers.c` (MODIFY - optional enhancement)
- [ ] Consider adding `discovery_mode` column to `gossip_peers` table to track how each peer was discovered:
  ```sql
  ALTER TABLE gossip_peers ADD COLUMN discovery_mode TEXT;
  ```
  - This allows tracking whether peer was discovered via Tailscale, DNS pattern, static config, or dynamic addition
  - Update `gossip_peers_add_or_update()` signature to accept optional `discovery_mode` parameter
  - Store discovery_mode when adding/updating peers
  - **Note**: This is optional - peers table works without it, but it's useful for debugging

**File**: `scripts/init.c` (MODIFY)
- [x] After initializing node database (after `gossip_store_init()` and `gossip_peers_init()`), store node configuration:
  - Call `nodes_insert_or_update()` with node_id, node_name, hostname, ports, discovery_mode, hostname_prefix, dns_domain
  - This persists node's own configuration in database during initialization
  - Uses defaults: `discovery_mode="static"`, `hostname_prefix=NULL`, `dns_domain=NULL` (will be updated when main.c loads the config)

**File**: `src/main.c` (MODIFY)
- [x] After loading config, store node configuration in database:
  - After `initialize_storage()` succeeds (database is initialized)
  - Call `nodes_insert_or_update()` with loaded config values (node_id, node_name, hostname, gossip_port, api_port, discovery_mode, hostname_prefix, dns_domain)
  - This ensures database has current node configuration even if config file is missing later
  - `#include "packages/sql/schema.h"` already present

### 1.7 Integrate Discovery into Main Application (COMPLETED)

**File**: `src/main.c` (MODIFY)
- [x] Update `start_gossip_service()` function (around line 207):
  - Added `#include "packages/discovery/discovery.h"`
  - Modified function signature to accept `NodeConfig* config` parameter
  - Call `discover_peers(&g_gossip_service, config)` BEFORE `bootstrap_known_peers()`
  - This ensures discovery-discovered peers are added first, then DB peers loaded
- [x] Handle case where discovery unavailable (graceful fallback to DB peers only)
- [x] Updated `main()` to pass `&config` to `start_gossip_service()`

**File**: `CMakeLists.txt` (MODIFY)
- [x] Discovery source files are automatically included via `file(GLOB_RECURSE SOURCES "src/*.c")`
  - Verified all discovery files are compiled: `discovery.c`, `tailscale_discovery.c`, `dns_pattern_discovery.c`, `static_discovery.c`

**Testing checkpoint**: 
- [x] Compile successfully with discovery module
- [x] Load config with discovery_mode field (already supported via config system)
- [x] Call `discover_peers()` without errors (graceful fallback implemented)

---

## Phase 2: Dynamic Peer Addition & Node Announcements

**Goal**: Enable nodes to discover each other dynamically when receiving messages and via announcements.

**Status**: ✅ **COMPLETED**

**Files modified**:
- `src/main.c` (MODIFIED - added `send_node_announcement()` and `handle_node_announcement()` functions)

**Testing checkpoint**: After Phase 2, you should be able to:
- Receive messages from unknown peers and add them automatically
- Send and receive node announcements
- Verify peers are stored in database

### 2.1 Dynamic Peer Addition on Message Receive

**File**: `src/main.c` (MODIFY - function `gossip_receive_handler()` around line 254)
- [x] Extract source address from `sockaddr_in` when receiving messages
- [x] Check if source peer exists in peer list (check `service->peers[]` array)
- [x] If unknown peer, resolve hostname:
  - **All modes**: Use reverse DNS via `gethostbyaddr()` (works with Tailscale MagicDNS, standard DNS, etc.)
  - Fallback to IP address if DNS resolution fails
- [x] Add peer dynamically via `gossip_service_add_peer(service, hostname, 9000)`
- [x] Store in database via `gossip_peers_add_or_update(hostname, 9000, 0, NULL)` for persistence
- [x] Log peer discovery events with discovery mode context

**Dependencies**:
- `gethostbyaddr()` for reverse DNS
- `gossip_service_add_peer()` from `src/packages/comm/gossip/gossip.c`
- `gossip_peers_add_or_update()` from `src/packages/sql/gossip_peers.c`

### 2.2 Node Announcement Helper Function

**File**: `src/main.c` (MODIFY - add new function)
- [x] Create helper function `send_node_announcement()`:
  - Creates envelope with `CONTENT_NODE_REGISTRATION` content type (type 40)
  - Includes node_pubkey, node_address (hostname), node_port (gossip_port, default 9000), api_port (default 8000), node_version in payload
  - Uses `NodeRegistration` protobuf message from `content.proto`
  - Signs with node's private key (use existing signing infrastructure)
  - Broadcasts to all currently known peers via `gossip_service_broadcast_envelope()`
- [x] Call `send_node_announcement()` after discovery completes (in `start_gossip_service()`)

**Dependencies**:
- `Tinyweb__NodeRegistration` from `generated/content.pb-c.h`
- `gossip_service_broadcast_envelope()` from `src/packages/comm/gossip/gossip.c`
- Node signing key from keystore

### 2.3 Handle Node Announcement Messages

**File**: `src/main.c` (MODIFY - function `gossip_receive_handler()` around line 254)
- [x] Check if envelope content_type is `CONTENT_NODE_REGISTRATION` (40)
- [x] If yes, extract node info from payload:
  - Parse `NodeRegistration` protobuf message
  - Extract node_pubkey, node_address (hostname), node_port (gossip_port), api_port from payload
- [x] Extract source address from `sockaddr_in` (for hostname verification)
- [x] Add announcer as peer if not already known via `gossip_service_add_peer()`
- [x] Store in database via `gossip_peers_add_or_update()`
- [x] This establishes bidirectional discovery: Node2 finds Node1 via API, Node1 learns Node2 via announcement

**Dependencies**:
- `Tinyweb__NodeRegistration` protobuf parsing
- `envelope_dispatch()` may already handle this - check `src/packages/comm/envelope_dispatcher.c`

**Testing checkpoint**:
- [ ] Send test message from unknown peer, verify it's added dynamically
- [ ] Send node announcement, verify recipient adds announcer as peer
- [ ] Verify peers persist in database after restart

---

## Phase 3: Config Generator & Docker Infrastructure

**Goal**: Create tools to generate node-specific configs and Docker setup files.

**Files to create**:
- `scripts/docker_config_generator.py` (NEW)
- `scripts/Dockerfile.node` (NEW)
- `scripts/docker_test_runner.sh` (NEW)

**Files to modify**:
- `scripts/init_tool.c` (MODIFY - add `--node-id` flag option)
- `scripts/configs/schema.json` (MODIFY - update for discovery mode)

**Testing checkpoint**: After Phase 3, you should be able to:
- Run config generator and produce node-specific configs
- Build Docker images
- Generate docker-compose files

### 3.1 Update Master Config Schema

**File**: `scripts/configs/schema.json` (MODIFY)
- [x] Update `docker` section to support discovery mode:
  ```json
  "docker": {
    "type": "object",
    "required": ["mode", "discovery"],
    "properties": {
      "mode": {
        "type": "string",
        "enum": ["production", "test"]
      },
      "discovery": {
        "type": "object",
        "required": ["mode", "hostname_prefix"],
        "properties": {
          "mode": {
            "type": "string",
            "enum": ["tailscale", "dns_pattern", "static"]
          },
          "hostname_prefix": {
            "type": "string",
            "description": "Prefix for hostname generation (e.g., 'tw_node', 'smithfam_tw_node')"
          },
          "dns_pattern": {
            "type": "object",
            "properties": {
              "pattern": {
                "type": "string",
                "description": "Optional DNS pattern (e.g., 'tw_node*.tinyweb.win')"
              },
              "domain": {
                "type": "string",
                "description": "Required if pattern not provided (e.g., 'tinyweb.win', 'duckdns.org')"
              }
            }
          }
        }
      }
    }
  }
  ```
- [x] For **Static mode**, nodes can include optional `hostname` and `peers` fields (already in schema)

### 3.2 Create Config Generator Script

**File**: `scripts/docker_config_generator.py` (CREATE)
- [ ] Create Python script to process master config
- [ ] Parse simplified config (nodes only, no ports, no peers in master config)
- [ ] Validate node count: Maximum 99 nodes per network (hostname format uses 2 digits: 01-99)
- [ ] Parse `extensions` array from config (if present) and validate against schema
  - **Note**: Extension service generation in docker-compose is reserved for dashboard
  - For now, just validate extension config structure matches schema
- [ ] Parse `docker.discovery.mode` from config: `"tailscale"` (default), `"dns_pattern"` (DNS wildcard query), or `"static"` (manual peer list)
- [ ] Use fixed ports for all nodes: `api_port = 8000`, `gossip_port = 9000` (each container has isolated network namespace, no conflicts)
- [ ] Generate hostnames based on discovery mode (extract node index from `node_01` → `1`, format as 2-digit zero-padded string: `"01"`):
  - **Tailscale mode**: Extract index from `node_01` → `1`, format as `"01"` (zero-padded), combine: `{hostname_prefix}{index}` → `tw_node01` (short hostname, MagicDNS resolves)
    - Examples: prefix `"tw_node"`, node `node_01` → `tw_node01`; node `node_22` → `tw_node22`; node `node_03` → `tw_node03`
    - Examples: prefix `"smithfam_tw_node"`, node `node_01` → `smithfam_tw_node01`; node `node_22` → `smithfam_tw_node22`
  - **DNS pattern mode**: Extract index from `node_01` → `1`, format as `"01"` (zero-padded), combine: `{hostname_prefix}{index}.{domain}` → `tw_node01.{domain}` (full domain)
    - Examples: prefix `"tw_node"`, domain `"duckdns.org"`, node `node_01` → `tw_node01.duckdns.org`; node `node_22` → `tw_node22.duckdns.org`
    - Examples: prefix `"smithfam_tw_node"`, domain `"duckdns.org"`, node `node_01` → `smithfam_tw_node01.duckdns.org`
  - **Static mode**: Use hostname from master config `nodes[].hostname` field (user-provided, `hostname_prefix` ignored)
    - Example: `"myhost.duckdns.org"` or `"smithfam_tw_node01.duckdns.org"` (user provides full hostname)
- [ ] For each node, generate node-specific config in `docker_configs/<node_id>/network_config.json` with:
  - `id`: from master config (e.g., `node_01`)
  - `name`: from master config (e.g., `Family Node 1`)
  - `hostname`: generated or from config (format depends on discovery mode)
  - `gossip_port`: `9000` (fixed)
  - `api_port`: `8000` (fixed)
  - `peers`: 
    - **Tailscale/DNS pattern modes**: empty array `[]` (dynamic discovery handles this)
    - **Static mode**: array of peer hostnames from master config (e.g., `["peer1.duckdns.org:9000", "peer2.duckdns.org:9000"]`)
  - `discovery_mode`: from master config (e.g., `"tailscale"`, `"dns_pattern"`, or `"static"`)
  - `dns_pattern` (if mode == "dns_pattern"): pattern like `"tw_node*.tinyweb.win"` or `"tw_node*.duckdns.org"`
- [ ] Create `docker_configs/<node_id>/state/` directory structure
- [ ] For each node, run `init_tool` to initialize state:
  - Option A: Modify `init_tool` to accept `--node-id <node_id>` to initialize only one node
  - Option B: Create temporary master config with single node, run `init_tool --config <temp_config>`, then copy `state/` to `docker_configs/node_XX/state/`
  - `init_tool` will generate keys, initialize database, seed users, etc. in the state directory
  - After initialization, copy generated `network_config.json` from state to `docker_configs/<node_id>/network_config.json` (or ensure init_tool saves it correctly)
- [ ] Generate `docker-compose.yml` (production)
- [ ] Generate `docker-compose.test.yml` (test mode)

**Command-line interface**:
```bash
python3 scripts/docker_config_generator.py \
  --master-config scripts/configs/network_config.json \
  --mode production  # or "test"
```

### 3.3 Update Init Tool for Single-Node Initialization

**File**: `scripts/init_tool.c` (MODIFY)
- [x] Add `--node-id <node_id>` command-line flag
- [x] When `--node-id` is provided:
  - Filter master config to only initialize the specified node
  - Initialize only that node's state directory
  - Save node-specific config to `state/network_config.json`

**Alternative**: If modifying init_tool is complex, use Option B (temporary single-node config)

### 3.4 Create Dockerfile

**File**: `scripts/Dockerfile.node` (CREATE)
- [x] Create Dockerfile with lightweight base (debian-slim)
- [x] Install runtime deps: libsodium, sqlite3, curl
- [x] Install discovery tools:
  - **Tailscale mode**: Install `tailscale` CLI (for `tailscale status` command)
  - **DNS pattern/Static modes**: No additional tools needed (uses standard DNS via `getaddrinfo()`)
  - Note: We install Tailscale CLI always since discovery mode is determined at runtime
- [x] Copy pre-built `tinyweb` binary from build directory
- [x] Note: Pre-initialized state directory is mounted as volume in docker-compose (from `docker_configs/<node_id>/state/` to `/app/state/`)
  - The state directory already contains `network_config.json` (saved by init_tool), database, keys, etc.
- [x] Set working directory to `/app`
- [x] Set entrypoint: `/app/tinyweb` (node_id comes from `TINYWEB_NODE_ID` env var, config loaded from `/app/state/network_config.json`)
- [x] Expose gossip_port 9000 (UDP) and api_port 8000 (TCP)
- [x] Note: Discovery mode is determined at runtime from config, not build time

**Build command**:
```bash
cmake -S . -B build
cmake --build build
# Then Dockerfile copies build/tinyweb to /app/tinyweb
```

### 3.5 Generate Docker Compose Files

**File**: `scripts/docker_config_generator.py` (MODIFY - add compose generation)
- [x] Generate node services based on discovery mode:
  
  **For Tailscale mode (default):**
  - `build`: reference to Dockerfile.node
  - `network_mode: service:tailscale_XXX` (share Tailscale sidecar's network namespace)
  - `environment`:
    - `TINYWEB_NODE_ID`: node index (e.g., `1`, `2`, `3`) - extracted from `node_01` → `1` (parse `node_XX` format, extract number)
    - `TINYWEB_DISCOVERY_MODE`: `tailscale` (optional, can be read from config)
  - `volumes`:
    - Mount `docker_configs/<node_id>/state/` to `/app/state/` (read-write for persistence)
  - `depends_on`:
    - `tailscale_XXX` with condition: `service_healthy` (wait for Tailscale to be connected)
  - `healthcheck`: HTTP GET on `http://localhost:8000/health` (or similar endpoint)
  - `restart`: `unless-stopped` (production) or `no` (test mode)
  - `ports` (optional, for external API access):
    - Map host port `8000 + index` to container port `8000` (e.g., `8001:8000`, `8002:8000`)
  
  **For DNS pattern mode:**
  - `build`: reference to Dockerfile.node
  - `network_mode: bridge` or `host` (user's choice, depends on tunnel/port forwarding setup)
  - `environment`:
    - `TINYWEB_NODE_ID`: node index
    - `TINYWEB_DISCOVERY_MODE`: `dns_pattern`
  - `volumes`: Same as Tailscale mode
  - `depends_on`: None (or tunnel container if user has one)
  - `healthcheck`: Same as Tailscale mode
  - `restart`: Same as Tailscale mode
  - `ports`: Expose ports based on user's setup (tunnel, port forwarding, etc.)
  
  **For Static mode (DuckDNS/direct IP):**
  - `build`: reference to Dockerfile.node
  - `network_mode: bridge` or `host` (user's choice)
  - `environment`:
    - `TINYWEB_NODE_ID`: node index
    - `TINYWEB_DISCOVERY_MODE`: `static`
  - `volumes`: Same as Tailscale mode
  - `depends_on`: None
  - `healthcheck`: Same as Tailscale mode
  - `restart`: Same as Tailscale mode
  - `ports`: Expose ports directly to host:
    - UDP `9000:9000` (gossip)
    - TCP `8000:8000` (API)
  - User must configure port forwarding on router for external access
  
- [x] **Generate Tailscale sidecar services** (only if discovery mode == "tailscale"):
  - `image: tailscale/tailscale:latest`
  - `environment`:
    - `TS_AUTHKEY`: `${TS_AUTHKEY}` (from environment variable)
    - `TS_HOSTNAME`: generated hostname (e.g., `tw_node01`, `tw_node02`)
    - `TS_STATE_DIR`: `/var/lib/tailscale`
  - `volumes`:
    - Named volume for Tailscale state: `tailscale_XXX_state:/var/lib/tailscale`
  - `cap_add`: `NET_ADMIN` (required for Tailscale)
  - `healthcheck`: Verify Tailscale is connected (e.g., `tailscale status --json` returns connected devices)
  - `restart`: `unless-stopped` (production) or `no` (test mode)
- [x] **Extension services (reserved for future dashboard integration)**:
  - If `extensions` array exists in master config, parse and validate (no services generated yet)
  - **For now**: Just parse and validate extension config, don't generate services yet (reserved for dashboard)
- [x] Test mode: Same setup as production, just different cleanup behavior (volumes removed on `down -v`)

**Output files**:
- `docker-compose.yml` (production services)
- `docker-compose.test.yml` (integration testing)

**Testing checkpoint**:
- [x] Run config generator, verify node-specific configs are created
- [x] Verify docker-compose files are generated correctly
- [x] Verify init_tool runs for each node and creates state directories
- [x] Verify extension config parsing (if extensions array present in master config)

**Extension Compatibility Notes** (for future dashboard integration):
- [x] Config generator should parse `extensions` array from master config (if present)
- [x] Validate extension config structure matches schema
- [ ] **Reserved for dashboard**: Extension service generation in docker-compose will be handled by dashboard
- [ ] **Reserved for dashboard**: Bridge communication protocol will be HTTP API:
  - Core exposes bridge endpoints at `/api/extensions/<extension_id>/*`
  - Extensions communicate with core via HTTP to `http://node_XX:8000/api/extensions/...`
  - Bridge endpoints will be implemented when dashboard is built
- [ ] **Reserved for dashboard**: Extension management API endpoints:
  - `GET /api/extensions` - List available/installed extensions
  - `POST /api/extensions/<id>/start` - Start extension service
  - `POST /api/extensions/<id>/stop` - Stop extension service
  - `GET /api/extensions/<id>/status` - Get extension status
  - These will be added to `src/packages/comm/gossipApi.c` when dashboard is implemented

---

## Phase 4: Integration Test Runner

**Goal**: Create automated test runner for Docker-based integration tests.

**File**: `scripts/docker_test_runner.sh` (CREATE)

- [x] Create shell script for test orchestration
- [x] Parse master config (or accept as argument)
- [x] Call config generator in test mode: `python3 scripts/docker_config_generator.py --master-config <config> --mode test`
- [x] Build Docker images: `docker-compose -f docker-compose.test.yml build`
- [x] Start containers: `docker-compose -f docker-compose.test.yml up -d`
- [x] Wait for health checks (poll until all healthy):
  - Check `docker-compose ps` for health status
  - Poll every 2 seconds, timeout after 120 seconds
- [x] Run test suite (optional argument):
  - If test script provided, run it
  - Otherwise, just verify nodes are running and healthy
- [x] Cleanup: `docker-compose -f docker-compose.test.yml down -v`
- [x] **Tailscale Ephemeral Key Warning**: Check if `TS_AUTHKEY` is set and warn user to use ephemeral keys for testing (devices auto-delete on disconnect, prevents hitting 100 device limit)

**Important Notes**:
- **CI/CD Integration**: CI/CD configuration is **optional** and **not required** for Phase 4. The test runner can be used manually or integrated into CI/CD pipelines later if desired.
- **Tailscale Ephemeral Keys**: For testing with Tailscale discovery mode, use **ephemeral auth keys** to prevent device accumulation. Ephemeral keys automatically remove devices when containers disconnect, preventing you from hitting Tailscale's 100 device limit. Create ephemeral keys in Tailscale Admin Console → Settings → Keys → "Ephemeral" checkbox.

**Usage**:
```bash
./scripts/docker_test_runner.sh [--config <config.json>] [--test-script <script.sh>]
```

**Testing checkpoint**:
- [x] Test runner script created and executable
- [x] Ephemeral key warning implemented
- [ ] Run test runner end-to-end
- [ ] Verify all containers start and become healthy
- [ ] Verify cleanup removes volumes

---

## Phase 5: Documentation & Cleanup

**Goal**: Update documentation and remove any remaining inconsistencies.

**Files to modify**:
- `README.md` (UPDATE)
- `initialization_tasks.txt` (UPDATE - mark completed tasks)

- [ ] Update `README.md` with Docker Compose deployment instructions
- [ ] Remove any outdated references to old setup methods
- [ ] Add examples for each discovery mode
- [ ] Document environment variables (`TS_AUTHKEY`, `TINYWEB_NODE_ID`)

---

## File Structure Summary

```
scripts/
├── docker_config_generator.py  # Main config processor (Phase 3)
├── Dockerfile.node              # Docker image definition (Phase 3)
├── docker_test_runner.sh        # Integration test orchestrator (Phase 4)
├── configs/
│   ├── network_config.json      # Master config (input)
│   └── schema.json              # Config schema (Phase 3)
└── docker_configs/              # Generated (gitignored, Phase 3)
    ├── node_01/
    │   ├── network_config.json  # Node-specific config (with hostname, ports)
    │   └── state/                # Pre-initialized with keys
    │       ├── network_config.json  # Same as above (copied by init_tool)
    │       ├── keys/             # Ed25519 keypairs
    │       └── storage/         # SQLite database (tinyweb.db)
    └── node_02/
        └── ...

src/packages/discovery/          # NEW (Phase 1)
├── discovery.h                  # Discovery interface
├── discovery.c                  # Router function
├── tailscale_discovery.c        # Tailscale implementation
├── dns_pattern_discovery.c     # DNS pattern implementation
└── static_discovery.c           # Static config implementation

# Generated in project root (Phase 3):
docker-compose.yml           # Production services
docker-compose.test.yml      # Integration testing
```

**Volume Mounting Strategy**:
- `docker_configs/<node_id>/state/` → `/app/state/` in container
- Container reads config from `/app/state/network_config.json`
- Container uses `/app/state/` for database and keys (persistent across restarts)

---

## Testing Strategy by Phase

### Phase 1 Testing
- [ ] Compile with discovery module (no errors)
- [ ] Load config with `discovery_mode` field (no errors)
- [ ] Call `discover_peers()` with each mode (may not find peers, but should not crash)
- [ ] Unit test: Mock Tailscale status JSON, verify parsing works
- [ ] Unit test: Mock DNS resolution, verify DNS pattern discovery works
- [ ] Unit test: Verify static discovery reads from config correctly

### Phase 2 Testing
- [ ] Send test message from unknown peer, verify it's added dynamically
- [ ] Send node announcement, verify recipient adds announcer as peer
- [ ] Verify peers persist in database after restart
- [ ] Test reverse DNS resolution (works with Tailscale MagicDNS)
- [ ] Test fallback to IP if DNS fails

### Phase 3 Testing
- [x] Run config generator, verify node-specific configs are created
- [x] Verify docker-compose files are generated correctly
- [x] Verify init_tool runs for each node and creates state directories
- [x] Verify generated configs have correct hostnames, ports, discovery_mode
- [ ] Test with all three discovery modes (tailscale, dns_pattern, static) - Manual testing needed with different configs

### Phase 4 Testing
- [ ] Run test runner end-to-end
- [ ] Verify all containers start and become healthy
- [ ] Verify cleanup removes volumes
- [ ] Test with 2-node network (Node1 starts first, Node2 discovers Node1)

### Phase 5 Testing
- [ ] Follow README instructions, verify they work
- [ ] Test all discovery modes from documentation

---

## Usage Examples

**Test mode (Tailscale discovery):**
```bash
export TS_AUTHKEY=tskey-auth-xxxxx  # Required for Tailscale mode
python3 scripts/docker_config_generator.py --master-config scripts/configs/network_config.json --mode test
docker-compose -f docker-compose.test.yml up -d
# Nodes discover each other via Tailscale API, then communicate
```

**Production mode (Tailscale discovery):**
```bash
export TS_AUTHKEY=tskey-auth-xxxxx
python3 scripts/docker_config_generator.py --master-config scripts/configs/network_config.json --mode production
docker-compose up -d
# Same discovery mechanism, just different cleanup behavior
```

**Production mode (Static discovery - DuckDNS/direct IP):**
```bash
# No special setup needed
python3 scripts/docker_config_generator.py --master-config scripts/configs/network_config.json --mode production
# Master config has: "docker": { "discovery": { "mode": "static" } }
# Nodes have hostnames and peers configured in config
docker-compose up -d
# Nodes read peers from config, resolve via DNS (works with DuckDNS, direct IP, etc.)
# Make sure ports 9000 (UDP) and 8000 (TCP) are forwarded on router
```

**Production mode (DNS pattern discovery):**
```bash
# No TS_AUTHKEY needed
python3 scripts/docker_config_generator.py --master-config scripts/configs/network_config.json --mode production
# Master config has: "docker": { "discovery": { "mode": "dns_pattern", "dns_pattern": { "pattern": "tw_node*.tinyweb.win" } } }
docker-compose up -d
# Nodes discover each other via DNS queries for tw_node*.tinyweb.win (or tw_node*.duckdns.org, etc.)
# Works with any DNS service that supports the pattern
```

---

## Discovery Flow Details

### Tailscale Mode

1. **Container startup sequence:**
   - Tailscale sidecar starts first
   - Tailscale authenticates with `TS_AUTHKEY`
   - Tailscale healthcheck passes (connected to tailnet)
   - Node container starts (depends on Tailscale sidecar being healthy)

2. **Node1 starts (first node):**
   - Loads config from `/app/state/network_config.json` (hostname: `tw_node01`, ports: 8000/9000, discovery_mode: `tailscale`)
   - Calls `discover_peers()` which routes to `discover_tailscale_peers()` with retry logic
   - Queries Tailscale API: `tailscale status --json`
   - Finds no other devices matching `{hostname_prefix}*` pattern (first node, e.g., `tw_node*` or `smithfam_tw_node*`)
   - Calls `bootstrap_known_peers()` (loads from DB, empty initially)
   - Starts listening on port 9000, no peers initially
   - Sends `CONTENT_NODE_REGISTRATION` announcement (no peers to send to, but ready for future)

3. **Node2 starts:**
   - Loads config (hostname: `tw_node02`, ports: 8000/9000, discovery_mode: `tailscale`)
   - Calls `discover_peers()` which routes to `discover_tailscale_peers()` with retry logic
   - Queries Tailscale API: `tailscale status --json`
   - Finds `tw_node01` device (Node1)
   - Adds `tw_node01:9000` as peer via `gossip_service_add_peer()`
   - Stores in database via `gossip_peers_add_or_update()`
   - Calls `bootstrap_known_peers()` (loads from DB, now includes Node1)
   - Starts listening on port 9000
   - Sends `CONTENT_NODE_REGISTRATION` announcement to Node1

4. **Node1 receives announcement:**
   - `gossip_receive_handler()` processes `CONTENT_NODE_REGISTRATION` message
   - Extracts Node2's hostname (`tw_node02`) and port (9000) from payload
   - Verifies source address matches announcement
   - Adds `tw_node02:9000` as peer dynamically via `gossip_service_add_peer()`
   - Stores in database via `gossip_peers_add_or_update()`

5. **Both nodes now know each other:**
   - Can send messages bidirectionally
   - Peer list persists in database
   - Future restarts will discover peers from both discovery backend and database

### DNS Pattern Mode

1. **Container startup sequence:**
   - Tunnel/port forwarding configured by user (Cloudflare tunnel, port forwarding, etc.)
   - Node container starts

2. **Node1 starts (first node):**
   - Loads config (hostname: `tw_node01.tinyweb.win`, ports: 8000/9000, discovery_mode: `dns_pattern`, pattern: `tw_node*.tinyweb.win`)
   - Calls `discover_peers()` which routes to `discover_dns_pattern_peers()`
   - Queries DNS for pattern `tw_node*.tinyweb.win` (or iterates through possible hostnames)
   - Finds no other nodes (first node, or DNS doesn't support wildcard enumeration)
   - Calls `bootstrap_known_peers()` (loads from DB, empty initially)
   - Starts listening on port 9000, no peers initially

3. **Node2 starts:**
   - Loads config (hostname: `tw_node02.tinyweb.win`, ports: 8000/9000, discovery_mode: `dns_pattern`)
   - Calls `discover_peers()` which routes to `discover_dns_pattern_peers()`
   - Queries DNS for pattern `tw_node*.tinyweb.win`
   - Finds `tw_node01.tinyweb.win` (Node1) via DNS resolution
   - Adds `tw_node01.tinyweb.win:9000` as peer
   - Rest of flow same as Tailscale mode

**Note**: DNS pattern discovery may be limited by DNS provider capabilities. Some providers don't support wildcard enumeration, so this mode may fall back to static discovery or require manual peer configuration.

---

## Key Features Checklist
- [x] Simple: Just docker-compose, no orchestration complexity
- [ ] Tailscale-based discovery: Always use Tailscale API to find peers
- [ ] Dynamic peer addition: Add peers when receiving messages
- [ ] Auto-configured: Ports and hostnames assigned automatically
- [ ] Fully containerized: Tailscale sidecars, no host setup
- [ ] Equal nodes: No primary/bootstrap node required
- [ ] Conflict-free naming: `tw_node01` avoids conflicts with other devices
- [ ] Bidirectional discovery: Node2 finds Node1 via API, Node1 learns Node2 via announcement

---

## Static Discovery Mode (DuckDNS / Direct IP)

### Overview
Support for users who expose ports directly (via port forwarding) and use dynamic DNS services like DuckDNS, or even direct IP addresses.

### How It Works
- **Discovery mode**: `"static"` reads peers from node-specific config's `peers` array
- **Hostname resolution**: Uses standard DNS (`getaddrinfo()`) - works with any DNS-resolvable hostname
- **No special infrastructure**: Just needs port forwarding and DNS (or direct IP)
- **Manual configuration**: Users provide peer hostnames in config

### Use Cases
- DuckDNS users: `myhost.duckdns.org:9000`
- Direct IP: `192.168.1.100:9000` (local network) or `1.2.3.4:9000` (public IP)
- Custom domains: `node1.example.com:9000`
- Mix of methods: Some peers via DuckDNS, some via direct IP

### Configuration Example
```json
{
  "network": { "name": "My Network" },
  "nodes": [
    {
      "id": "node_01",
      "name": "Home Server",
      "hostname": "myhost.duckdns.org"
    },
    {
      "id": "node_02",
      "name": "Raspberry Pi",
      "hostname": "pi.duckdns.org",
      "peers": ["myhost.duckdns.org:9000"]
    }
  ],
  "docker": {
    "discovery": {
      "mode": "static"
    }
  }
}
```

### Implementation
- Already planned in Phase 1: `discover_static_peers()` function
- Reads from `config->peers` array
- Uses `getaddrinfo()` for DNS resolution (works with any hostname)
- No special dependencies needed

### Benefits
- **No infrastructure required**: Works with any DNS service or direct IP
- **Simple setup**: Just configure peer hostnames
- **Flexible**: Mix DuckDNS, direct IPs, custom domains
- **Works everywhere**: Home networks, VPS, anywhere with port forwarding

---

## DNS Pattern Discovery Mode

### Overview
Support for DNS-based peer discovery using wildcard patterns. Works with any DNS service (Cloudflare, DuckDNS, custom domains, etc.) that can resolve hostnames matching a pattern.

### How It Works
- **Discovery mode**: `"dns_pattern"` queries DNS for hostnames matching a pattern
- **Pattern format**: `{prefix}*.{domain}` (e.g., `tw_node*.tinyweb.win` or `tw_node*.duckdns.org`)
- **DNS resolution**: Uses standard DNS queries - works with any DNS provider
- **Limitation**: Not all DNS providers support wildcard enumeration, may need to iterate through possible hostnames

### Use Cases
- Cloudflare tunnels: `tw_node*.tinyweb.win`
- DuckDNS: `tw_node*.duckdns.org` (if DNS supports pattern queries)
- Custom domains: `node*.example.com`
- Any DNS service with predictable hostname patterns

### Implementation Notes
- DNS wildcard enumeration may not be supported by all providers
- Fallback options:
  1. Iterate through possible hostnames (tw_node01, tw_node02, etc.) and resolve each
  2. Fall back to static discovery if DNS pattern fails
  3. Use static mode for providers that don't support wildcard queries

### Benefits
- Works with any DNS service (not Cloudflare-specific)
- Automatic peer discovery if DNS supports it
- Full domain names for better DNS management
- Each node can run independently on different machines

### When to Use
- **DNS pattern mode**: When you have a predictable hostname pattern and DNS supports wildcard queries
- **Static mode**: When DNS doesn't support wildcards, or you prefer explicit peer configuration
- **Tailscale mode**: When you want automatic discovery without DNS configuration

---

## Extension Compatibility (Reserved for Future Dashboard Integration)

### Overview
The initialization system is designed to be compatible with future extension services (Jellyfin, Immich, etc.) that will be managed by a dashboard. This section documents the compatibility requirements without implementing the full extension infrastructure.

### Config Schema Support
- [x] **Extensions array in master config**: Added optional `extensions` array to `scripts/configs/schema.json`
- [x] **Extension metadata structure**: Schema includes `id`, `name`, `docker_image`, `ports`, `bridge_endpoint`, `enabled`
- [ ] **Config generator parsing**: `docker_config_generator.py` should parse and validate extensions array (if present)
  - **Note**: Service generation will be handled by dashboard, not config generator

### Docker Compose Compatibility
- [ ] **Docker bridge network**: Core nodes use Docker bridge network to allow extension communication
- [ ] **Service naming convention**: Extensions will use naming pattern `extension_<id>_<node_id>` (e.g., `extension_jellyfin_node_01`)
- [ ] **Network isolation**: Extensions share bridge network with core node for communication
- [ ] **Port management**: Extensions use ports from config, avoid conflicts with core ports (8000, 9000)

### Bridge Communication Protocol (Reserved)
**Future implementation** - Core ↔ Extension communication:
- **Protocol**: HTTP REST API
- **Core endpoint**: `http://node_XX:8000/api/extensions/<extension_id>/*`
- **Extension endpoint**: Extensions call core at `http://node_XX:8000/api/extensions/...`
- **Authentication**: TBD (will be implemented with dashboard)
- **Message routing**: Core can route gossip messages to extensions via bridge API

### Extension Management API Endpoints (Reserved)
**Future implementation** - Dashboard will use these endpoints:
- `GET /api/extensions` - List available/installed extensions
- `POST /api/extensions/<id>/start` - Start extension service (triggers docker-compose service start)
- `POST /api/extensions/<id>/stop` - Stop extension service
- `GET /api/extensions/<id>/status` - Get extension health/status
- `POST /api/extensions/<id>/configure` - Update extension configuration
- **Implementation location**: `src/packages/comm/gossipApi.c` (to be added when dashboard is built)

### App Distribution (Reserved)
**Future implementation** - Network-wide app distribution:
- **App registry**: Metadata about available apps (name, version, repo URL, permissions)
- **Distribution protocol**: Apps pushed to phones via repo URL (managed by dashboard)
- **Network-wide sync**: App availability synced across nodes via gossip protocol
- **Implementation**: Will use existing gossip message types for app metadata distribution

### Current Status
- ✅ Config schema supports extensions array
- ✅ Docker Compose structure compatible with extension services
- ⏳ Extension service generation: Reserved for dashboard
- ⏳ Bridge communication: Reserved for dashboard
- ⏳ Extension management API: Reserved for dashboard
- ⏳ App distribution: Reserved for dashboard

**Note**: The initialization plan focuses on core node setup. Extension infrastructure will be implemented when the dashboard is built. The current plan ensures compatibility so extensions can be added without breaking changes.
